[
  {
    "objectID": "pages/a-study-on-the-impact-of-feature-scaling.html",
    "href": "pages/a-study-on-the-impact-of-feature-scaling.html",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "In the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use the Wine dataset from scikit-learn, a frequently employed dataset for classification tasks, to demonstrate the effects of these scaling methods. The Wine dataset contains information about different wines and their classification into one of three classes."
  },
  {
    "objectID": "pages/a-study-on-the-impact-of-feature-scaling.html#introduction",
    "href": "pages/a-study-on-the-impact-of-feature-scaling.html#introduction",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "In the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use the Wine dataset from scikit-learn, a frequently employed dataset for classification tasks, to demonstrate the effects of these scaling methods. The Wine dataset contains information about different wines and their classification into one of three classes."
  },
  {
    "objectID": "pages/a-study-on-the-impact-of-feature-scaling.html#importing-necessary-libraries",
    "href": "pages/a-study-on-the-impact-of-feature-scaling.html#importing-necessary-libraries",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nBefore we begin, we need to import the required libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "pages/a-study-on-the-impact-of-feature-scaling.html#loading-the-wine-dataset",
    "href": "pages/a-study-on-the-impact-of-feature-scaling.html#loading-the-wine-dataset",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Loading the Wine Dataset",
    "text": "Loading the Wine Dataset\nWe start by loading the Wine dataset and inspecting its structure.\n\n# Load the Wine dataset\nwine = load_wine()\n\n# Create a DataFrame for the dataset\nwine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\n\n# Display the first few rows of the dataset\nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0.0\n\n\n\n\n\n\n\nThe Wine dataset consists of various features related to wine properties and a ‘target’ column indicating the class of the wine."
  },
  {
    "objectID": "pages/a-study-on-the-impact-of-feature-scaling.html#data-preprocessing",
    "href": "pages/a-study-on-the-impact-of-feature-scaling.html#data-preprocessing",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we proceed with feature scaling, we need to split the data into training and testing sets. Additionally, to make our study more robust and thorough, we will create a noisy version of the Wine dataset by adding random noise to the feature values. This noisy dataset will introduce variations that can better showcase the effects of different scaling methods on classification model performance.\n\n# Split the data into features (X) and target (y)\nX = wine.data\ny = wine.target\n\n# Adding random noise to the dataset\nnp.random.seed(42)\nnoise = np.random.normal(0, 0.2, size=X.shape)\nX_noisy = X + noise\n\n# Split the noisy data into training and testing sets\nX_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "pages/a-study-on-the-impact-of-feature-scaling.html#feature-scaling-methods",
    "href": "pages/a-study-on-the-impact-of-feature-scaling.html#feature-scaling-methods",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Feature Scaling Methods",
    "text": "Feature Scaling Methods\n\n1. Standard Scaler\nThe Standard Scaler (\\(SS\\)) transforms the data so that it has a mean (\\(\\mu\\)) of 0 and a standard deviation (\\(\\sigma\\)) of 1. This method assumes that the data is normally distributed. The transformation is given by:\n\\[\nSS(x) = \\frac{x - \\mu}{\\sigma}\n\\]\nwhere \\(x\\) is the original feature vector, \\(\\mu\\) is the mean of the feature vector, and \\(\\sigma\\) is the standard deviation of the feature vector.\n\n# Initialize the Standard Scaler\nstandard_scaler = StandardScaler()\n\n# Fit and transform the training data\nX_train_standard = standard_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_standard = standard_scaler.transform(X_test_noisy)\n\n\n\n2. Min-max Scaler\nThe Min-max Scaler (\\(MMS\\)) scales the data to a specific range, typically between 0 and 1. It is suitable for data that does not follow a normal distribution. The transformation is given by:\n\\[\nMMS(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\n\\]\nwhere \\(x\\) is the original feature vector, \\(x_{min}\\) is the smallest value in the feature vector, and \\(x_{max}\\) is the largest value in the feature vector.\n\n# Initialize the Min-max Scaler\nmin_max_scaler = MinMaxScaler()\n\n# Fit and transform the training data\nX_train_minmax = min_max_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_minmax = min_max_scaler.transform(X_test_noisy)\n\n\n\n3. Maximum Absolute Scaler\nThe Maximum Absolute Scaler (\\(MAS\\)) scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. It does not shift/center the data, and thus does not destroy any sparsity. The transformation is given by:\n\\[\nMAS(x) = \\frac{x}{|x_{max}|}\n\\]\nwhere \\(x\\) is the original feature vector, and \\(x_{max, abs}\\) is the maximum absolute value in the feature vector.\n\n# Initialize the Maximum Absolute Scaler\nmax_abs_scaler = MaxAbsScaler()\n\n# Fit and transform the training data\nX_train_maxabs = max_abs_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_maxabs = max_abs_scaler.transform(X_test_noisy)\n\n\n\n4. Robust Scaler\nThe Robust Scaler (\\(RS\\)) scales the data using the median (\\(Q_2\\)) and the interquartile range (\\(IQR\\), \\(Q_3 - Q_1\\)), making it robust to outliers. The transformation is given by:\n\\[\nRS(x) = \\frac{x - Q_2}{IQR}\n\\]\nwhere \\(x\\) is the original feature vector, \\(Q_2\\) is the median of the feature vector, and \\(IQR\\) is the interquartile range of the feature vector.\n\n# Initialize the Robust Scaler\nrobust_scaler = RobustScaler()\n\n# Fit and transform the training data\nX_train_robust = robust_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_robust = robust_scaler.transform(X_test_noisy)\n\n\n\n5. Quantile Transformer\nThe Quantile Transformer (\\(QT\\)) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n\\[\nQT(x) = F^{-1}(F(x))\n\\]\nwhere \\(F(x)\\) is the cumulative distribution function of the data, and \\(F^{-1}\\) is the inverse function of \\(F\\).\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# Initialize the Quantile Transformer\nquantile_transformer = QuantileTransformer(output_distribution='normal')\n\n# Fit and transform the training data\nX_train_quantile = quantile_transformer.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_quantile = quantile_transformer.transform(X_test_noisy)"
  },
  {
    "objectID": "pages/a-study-on-the-impact-of-feature-scaling.html#classification-models",
    "href": "pages/a-study-on-the-impact-of-feature-scaling.html#classification-models",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Classification Models",
    "text": "Classification Models\nWe will now compare the performance of two classification models, Random Forest and Support Vector Machine (SVM), on the different scaled datasets. For each scaling method, we will train and evaluate both models.\n\n# Initialize the Random Forest classifier\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Initialize the SVM classifier\nsvm_classifier = SVC(random_state=42)\n\n# Lists to store accuracy scores\naccuracy_scores = []\n\n# Loop through each scaled dataset and evaluate the models\nfor X_train_scaled, X_test_scaled, scaler_name in zip(\n    [X_train_noisy, X_train_standard, X_train_minmax, X_train_maxabs, X_train_robust, X_train_quantile],\n    [X_test_noisy, X_test_standard, X_test_minmax, X_test_maxabs, X_test_robust, X_test_quantile],\n    [\"No Scaling\", \"Standard Scaler\", \"Min-max Scaler\", \"Maximum Absolute Scaler\", \"Robust Scaler\", \"Quantile Transformer\"]\n):\n    # Train the Random Forest model\n    rf_classifier.fit(X_train_scaled, y_train)\n    rf_predictions = rf_classifier.predict(X_test_scaled)\n    \n    # Train the SVM model\n    svm_classifier.fit(X_train_scaled, y_train)\n    svm_predictions = svm_classifier.predict(X_test_scaled)\n    \n    # Calculate accuracy scores for both models\n    rf_accuracy = accuracy_score(y_test, rf_predictions)\n    svm_accuracy = accuracy_score(y_test, svm_predictions)\n    \n    # Store the accuracy scores for comparison\n    accuracy_scores.append([scaler_name, rf_accuracy, svm_accuracy])"
  },
  {
    "objectID": "pages/a-study-on-the-impact-of-feature-scaling.html#results-and-discussion",
    "href": "pages/a-study-on-the-impact-of-feature-scaling.html#results-and-discussion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet’s analyze the results of our experiment and discuss the impact of different scaling methods on classification models.\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(accuracy_scores, columns=['Scaling Method', 'Random Forest Accuracy', 'SVM Accuracy'])\nresults_df\n\n\n\n\n\n\n\n\nScaling Method\nRandom Forest Accuracy\nSVM Accuracy\n\n\n\n\n0\nNo Scaling\n1.0\n0.805556\n\n\n1\nStandard Scaler\n1.0\n1.000000\n\n\n2\nMin-max Scaler\n1.0\n1.000000\n\n\n3\nMaximum Absolute Scaler\n1.0\n1.000000\n\n\n4\nRobust Scaler\n1.0\n1.000000\n\n\n5\nQuantile Transformer\n1.0\n1.000000"
  },
  {
    "objectID": "pages/a-study-on-the-impact-of-feature-scaling.html#evaluation-of-results",
    "href": "pages/a-study-on-the-impact-of-feature-scaling.html#evaluation-of-results",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Evaluation of Results",
    "text": "Evaluation of Results\nThe output from the notebook provides accuracy scores for two classification models, Random Forest and Support Vector Machine (SVM), using different feature scaling methods. Here’s a summary of the results:\n\nNo Scaling: Without any scaling, the Random Forest model achieved perfect accuracy (1.0), while the SVM model’s accuracy was significantly lower (approximately 0.8056). This disparity demonstrates the influence of feature scaling on SVM, which is sensitive to the range of feature values.\nStandard Scaler: The Standard Scaler, which assumes a normal distribution of data, yielded perfect accuracy (1.0) for both models. This indicates that the features in the Wine dataset are likely normally distributed, and the scaling effectively standardized the data, leading to improved SVM performance.\nMin-max Scaler, Maximum Absolute Scaler, Robust Scaler, and Quantile Transformer: These methods also resulted in perfect accuracy (1.0) for both models. These results demonstrate that scaling the data to a specific range (Min-max Scaler and Maximum Absolute Scaler), making the scaling robust to outliers (Robust Scaler), or applying a non-linear transformation to map data to a uniform or normal distribution (Quantile Transformer) can significantly improve the performance of SVM. It’s worth noting that the Random Forest’s performance remained consistently high regardless of the scaling method, which is consistent with its insensitivity to the scale of features."
  },
  {
    "objectID": "pages/a-study-on-the-impact-of-feature-scaling.html#conclusion",
    "href": "pages/a-study-on-the-impact-of-feature-scaling.html#conclusion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Conclusion:",
    "text": "Conclusion:\nIn conclusion, this notebook provides a comprehensive study on the impact of feature scaling on classification models. It demonstrates that the choice of feature scaling method can significantly influence the performance of a model, especially for models like SVM that are sensitive to the range of feature values.\nWithout scaling, SVM’s performance was significantly lower compared to other methods. However, with the application of different scaling methods, SVM’s performance improved drastically, achieving perfect accuracy. This highlights the importance of feature scaling in preprocessing, particularly when using models sensitive to the scale of input features.\nThe Standard Scaler, Min-max Scaler, Maximum Absolute Scaler, Robust Scaler, and Quantile Transformer all proved to be effective for the noisy Wine dataset. However, the effectiveness of a scaling method can vary based on the characteristics of the dataset and the specific machine learning model being used.\nWhen working with real-world datasets, it’s essential to experiment with different scaling techniques and select the one that best fits the data’s distribution and the requirements of the machine learning model. This decision should be data-driven and based on a thorough understanding of the data’s characteristics.\nThis experiment underscores the importance of feature scaling as a preprocessing step and the need to consider the specific scaling method in the broader context of machine learning tasks."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kaggle Playground Submissions",
    "section": "",
    "text": "This repository contains notebooks of my submissions for Kaggle Playground competitions. The Kaggle Playgrounds often serve as excellent learning grounds for data science enthusiasts to explore various datasets, practice machine learning, and compete with peers.\nClick on the “Notebooks” section in the sidebar to check them out."
  },
  {
    "objectID": "pages/pg-s3e26.html",
    "href": "pages/pg-s3e26.html",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "",
    "text": "Micrograph of PBC showing bile duct inflammation and injury, H&E stain\n\n\n\n\nCirrhosis stands as a severe consequence of liver diseases, where extensive scarring compromises liver function. The dataset at hand is inspired from a dataset by the Mayo Clinic’s trial addressing primary biliary cirrhosis (PBC) during 1974-1984. The original data encompasses crucial insights gleaned from 424 PBC patients, marking a pivotal endeavor in understanding the clinical landscape of this condition.\nThe original dataset, meticulously gathered as part of a randomized placebo-controlled trial involving D-penicillamine, presents a comprehensive array of covariates. Each entry comprises fundamental patient attributes, ranging from clinical measurements to key indicators of liver health.\n\n\n\nThe primary aim of this notebook is to harness machine learning methodologies to predict the status of PBC patients based on a diverse set of features. The ‘Status’ variable, delineated as C (censored), CL (censored due to liver tx), or D (death), serves as the focal point for predictive modeling. Leveraging the patient-specific attributes provided, the goal is to construct robust predictive models capable of discerning and forecasting patient outcomes within the scope of the study period.\n\n\n\n\nSize: The dataset contains information on 7905 PBC patients, amalgamating both trial participants and supplementary cases with recorded measurements.\nFeatures: Comprising 19 attributes encompassing a spectrum of patient-related variables, including demographic data, clinical measurements, and indicators of liver health.\nContext: The features encapsulate critical facets of patient health, such as age, sex, presence of specific symptoms (ascites, hepatomegaly, spiders, edema), laboratory test results (bilirubin, cholesterol, albumin, copper, alkaline phosphatase, SGOT, triglycerides, platelets, prothrombin), and histologic staging of the disease.\n\nThroughout this notebook, we embark on a journey through data exploration, preprocessing, model selection, and evaluation, culminating in the development of predictive models aimed at discerning and forecasting the status of PBC patients.\nLet’s delve into the intricate realm of predictive modeling in the context of primary biliary cirrhosis, unraveling insights and patterns within this comprehensive dataset."
  },
  {
    "objectID": "pages/pg-s3e26.html#introduction",
    "href": "pages/pg-s3e26.html#introduction",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "",
    "text": "Cirrhosis stands as a severe consequence of liver diseases, where extensive scarring compromises liver function. The dataset at hand is inspired from a dataset by the Mayo Clinic’s trial addressing primary biliary cirrhosis (PBC) during 1974-1984. The original data encompasses crucial insights gleaned from 424 PBC patients, marking a pivotal endeavor in understanding the clinical landscape of this condition.\nThe original dataset, meticulously gathered as part of a randomized placebo-controlled trial involving D-penicillamine, presents a comprehensive array of covariates. Each entry comprises fundamental patient attributes, ranging from clinical measurements to key indicators of liver health."
  },
  {
    "objectID": "pages/pg-s3e26.html#objective",
    "href": "pages/pg-s3e26.html#objective",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "",
    "text": "The primary aim of this notebook is to harness machine learning methodologies to predict the status of PBC patients based on a diverse set of features. The ‘Status’ variable, delineated as C (censored), CL (censored due to liver tx), or D (death), serves as the focal point for predictive modeling. Leveraging the patient-specific attributes provided, the goal is to construct robust predictive models capable of discerning and forecasting patient outcomes within the scope of the study period."
  },
  {
    "objectID": "pages/pg-s3e26.html#dataset-overview",
    "href": "pages/pg-s3e26.html#dataset-overview",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "",
    "text": "Size: The dataset contains information on 7905 PBC patients, amalgamating both trial participants and supplementary cases with recorded measurements.\nFeatures: Comprising 19 attributes encompassing a spectrum of patient-related variables, including demographic data, clinical measurements, and indicators of liver health.\nContext: The features encapsulate critical facets of patient health, such as age, sex, presence of specific symptoms (ascites, hepatomegaly, spiders, edema), laboratory test results (bilirubin, cholesterol, albumin, copper, alkaline phosphatase, SGOT, triglycerides, platelets, prothrombin), and histologic staging of the disease.\n\nThroughout this notebook, we embark on a journey through data exploration, preprocessing, model selection, and evaluation, culminating in the development of predictive models aimed at discerning and forecasting the status of PBC patients.\nLet’s delve into the intricate realm of predictive modeling in the context of primary biliary cirrhosis, unraveling insights and patterns within this comprehensive dataset."
  },
  {
    "objectID": "pages/pg-s3e26.html#dataset-snapshot",
    "href": "pages/pg-s3e26.html#dataset-snapshot",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Dataset Snapshot",
    "text": "Dataset Snapshot\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = pd.read_csv('/kaggle/input/playground-series-s3e26/train.csv', index_col='id')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s3e26/test.csv', index_col='id')\norig_df = pd.read_csv(\"/kaggle/input/cirrhosis-prediction-dataset/cirrhosis.csv\", index_col=\"ID\")\ndata = pd.concat([orig_df, data], axis=0)\ndata.head()\n\n\n\n\n\n\n\n\nN_Days\nStatus\nDrug\nAge\nSex\nAscites\nHepatomegaly\nSpiders\nEdema\nBilirubin\nCholesterol\nAlbumin\nCopper\nAlk_Phos\nSGOT\nTryglicerides\nPlatelets\nProthrombin\nStage\n\n\n\n\n1\n400\nD\nD-penicillamine\n21464\nF\nY\nY\nY\nY\n14.5\n261.0\n2.60\n156.0\n1718.0\n137.95\n172.0\n190.0\n12.2\n4.0\n\n\n2\n4500\nC\nD-penicillamine\n20617\nF\nN\nY\nY\nN\n1.1\n302.0\n4.14\n54.0\n7394.8\n113.52\n88.0\n221.0\n10.6\n3.0\n\n\n3\n1012\nD\nD-penicillamine\n25594\nM\nN\nN\nN\nS\n1.4\n176.0\n3.48\n210.0\n516.0\n96.10\n55.0\n151.0\n12.0\n4.0\n\n\n4\n1925\nD\nD-penicillamine\n19994\nF\nN\nY\nY\nS\n1.8\n244.0\n2.54\n64.0\n6121.8\n60.63\n92.0\n183.0\n10.3\n4.0\n\n\n5\n1504\nCL\nPlacebo\n13918\nF\nN\nY\nY\nN\n3.4\n279.0\n3.53\n143.0\n671.0\n113.15\n72.0\n136.0\n10.9\n3.0\n\n\n\n\n\n\n\nThe initial entries of the dataset, as revealed by the train.head() command, offer a glimpse into the fundamental attributes and features characterizing the primary biliary cirrhosis (PBC) dataset. These entries encompass a diverse range of patient-related information, providing crucial insights into the condition and status of the individuals under study.\nAttributes Overview:\n\nID: A unique identifier assigned to each patient within the dataset.\nN_Days: The number of days between registration and the earlier occurrence of death, liver transplantation, or the study analysis time in July 1986.\nStatus: Categorization of the patient’s status denoted as C (censored), CL (censored due to liver tx), or D (death).\nDrug: Indicates the type of drug administered to the patient, specifying either D-penicillamine or Placebo.\nAge: Age of the patient represented in days.\nSex: Gender classification denoted as M (male) or F (female).\nAscites: Presence or absence of ascites, indicated by Y (Yes) or N (No), respectively.\nHepatomegaly: Identification of hepatomegaly, denoted by Y (Yes) or N (No).\nSpiders: Presence or absence of spiders, indicating Y (Yes) or N (No), respectively.\nEdema: Details the presence and management of edema, categorized as S (edema present without diuretics, or edema resolved by diuretics), Y (edema despite diuretic therapy), or N (no edema and no diuretic therapy for edema).\nBilirubin: Serum bilirubin levels measured in mg/dl.\nCholesterol: Serum cholesterol levels measured in mg/dl.\nAlbumin: Albumin levels in gm/dl.\nCopper: Urine copper levels recorded in ug/day.\nAlk_Phos: Alkaline phosphatase levels in U/liter.\nSGOT: SGOT (Serum Glutamic Oxaloacetic Transaminase) levels measured in U/ml.\nTryglicerides: Triglyceride levels measured in mg/dl.\nPlatelets: Platelet count per cubic ml/1000.\nProthrombin: Prothrombin time measured in seconds (s).\nStage: Histologic stage of the disease categorized as 1, 2, 3, or 4.\n\nThis snapshot serves as a foundational view of the dataset, highlighting the diverse array of attributes and their corresponding values that will be further explored, analyzed, and utilized in the predictive modeling and analysis undertaken in this notebook."
  },
  {
    "objectID": "pages/pg-s3e26.html#dataset-information-summary",
    "href": "pages/pg-s3e26.html#dataset-information-summary",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Dataset Information Summary",
    "text": "Dataset Information Summary\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 8323 entries, 1 to 7904\nData columns (total 19 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   N_Days         8323 non-null   int64  \n 1   Status         8323 non-null   object \n 2   Drug           8217 non-null   object \n 3   Age            8323 non-null   int64  \n 4   Sex            8323 non-null   object \n 5   Ascites        8217 non-null   object \n 6   Hepatomegaly   8217 non-null   object \n 7   Spiders        8217 non-null   object \n 8   Edema          8323 non-null   object \n 9   Bilirubin      8323 non-null   float64\n 10  Cholesterol    8189 non-null   float64\n 11  Albumin        8323 non-null   float64\n 12  Copper         8215 non-null   float64\n 13  Alk_Phos       8217 non-null   float64\n 14  SGOT           8217 non-null   float64\n 15  Tryglicerides  8187 non-null   float64\n 16  Platelets      8312 non-null   float64\n 17  Prothrombin    8321 non-null   float64\n 18  Stage          8317 non-null   float64\ndtypes: float64(10), int64(2), object(7)\nmemory usage: 1.3+ MB\n\n\nThe dataset comprises a total of 7905 entries, each corresponding to a distinct patient record. There are 19 columns, each representing specific attributes or features related to the patients’ medical history and condition.\nInsights from the Data Columns:\n\nCategorical Variables: Several columns in the dataset are categorized as object data type, indicating categorical information. These columns include ‘Drug,’ ‘Ascites,’ ‘Hepatomegaly,’ ‘Spiders,’ ‘Edema,’ and ‘Status.’ Notably, ‘Drug,’ ‘Ascites,’ ‘Hepatomegaly,’ and ‘Spiders’ exhibit non-null counts of 7905, implying no missing values in these categorical attributes.\nNumerical Variables: The dataset also contains numerical variables, delineated by float64 and int64 data types. These columns encompass clinical measurements, patient-specific details like age, laboratory test results, and disease stage. Interestingly, attributes such as ‘Cholesterol,’ ‘Copper,’ ‘Alk_Phos,’ ‘SGOT,’ ‘Tryglicerides,’ ‘Platelets,’ ‘Prothrombin,’ and ‘Stage’ also exhibit non-null counts of 7905, suggesting a lack of missing values in these numerical features.\n\nMemory Utilization:\nThe dataset’s memory usage is estimated to be over 1.2 MB, highlighting the computational footprint required to store this dataset in memory.\nData Completeness and Challenges:\nSurprisingly, based on the updated information, it appears that there are no missing values across any of the columns in this dataset. This completeness diminishes the need for immediate data imputation or handling missing values during the preprocessing stage."
  },
  {
    "objectID": "pages/pg-s3e26.html#statistical-summary-of-the-dataset",
    "href": "pages/pg-s3e26.html#statistical-summary-of-the-dataset",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Statistical Summary of the Dataset",
    "text": "Statistical Summary of the Dataset\n\ndata.describe()\n\n\n\n\n\n\n\n\nN_Days\nAge\nBilirubin\nCholesterol\nAlbumin\nCopper\nAlk_Phos\nSGOT\nTryglicerides\nPlatelets\nProthrombin\nStage\n\n\n\n\ncount\n8323.000000\n8323.000000\n8323.000000\n8189.000000\n8323.000000\n8215.000000\n8217.000000\n8217.000000\n8187.000000\n8312.000000\n8321.000000\n8317.000000\n\n\nmean\n2024.528776\n18381.192359\n2.625940\n351.219074\n3.545767\n84.421546\n1823.044883\n114.906530\n115.662636\n264.827238\n10.634575\n3.032103\n\n\nstd\n1094.968696\n3686.832308\n3.847146\n196.775246\n0.350697\n76.327480\n1913.388685\n49.134067\n53.037660\n88.039809\n0.795707\n0.867235\n\n\nmin\n41.000000\n9598.000000\n0.300000\n120.000000\n1.960000\n4.000000\n289.000000\n26.350000\n33.000000\n62.000000\n9.000000\n1.000000\n\n\n25%\n1220.000000\n15574.000000\n0.700000\n248.000000\n3.350000\n39.000000\n834.000000\n75.950000\n84.000000\n210.000000\n10.000000\n2.000000\n\n\n50%\n1831.000000\n18713.000000\n1.100000\n299.000000\n3.580000\n63.000000\n1181.000000\n108.500000\n104.000000\n264.000000\n10.600000\n3.000000\n\n\n75%\n2689.000000\n20684.000000\n3.000000\n392.000000\n3.770000\n102.000000\n1857.000000\n137.950000\n139.000000\n316.000000\n11.000000\n4.000000\n\n\nmax\n4795.000000\n28650.000000\n28.000000\n1775.000000\n4.640000\n588.000000\n13862.400000\n457.250000\n598.000000\n721.000000\n18.000000\n4.000000\n\n\n\n\n\n\n\nThe statistical summary provides comprehensive information regarding the numerical columns within the dataset, offering insights into the central tendencies, dispersion, and distribution of various clinical measurements and patient-specific attributes.\nKey Observations:\n\nCount: Each column’s count indicates the number of non-null entries for the corresponding attribute. Consistent counts across columns suggest no missing values for any of the numerical features.\nCentral Tendencies:\n\nN_Days (Number of Days): The average time between registration and events (death, transplantation, or study analysis) stands at approximately 2030.17 days, with a wide range from 41 days to 4795 days, highlighting significant variability in the duration between events across patients.\nAge: The average age in days is approximately 18373.15, ranging from a minimum of 9598 days to a maximum of 28650 days, demonstrating a diverse age distribution among patients in the dataset.\nClinical Measurements: Various clinical measurements such as bilirubin, cholesterol, albumin, copper, alkaline phosphatase (Alk_Phos), SGOT, triglycerides, platelets, prothrombin, and disease stage (Stage) exhibit diverse mean values and ranges, indicating variability in patient conditions and responses to treatment.\n\nDispersion:\n\nStandard Deviation: Standard deviations for attributes like bilirubin, cholesterol, Alk_Phos, SGOT, and others are relatively high, signifying considerable variability or spread of values around their respective means across the patient population.\n\nMinimum, Maximum, and Quartiles:\n\nThe minimum and maximum values denote the range observed within each attribute, showcasing the extremes of values in the dataset.\nQuartiles (25%, 50%, and 75%) offer insights into the distribution of values, showcasing the spread of data and potential presence of outliers within each numerical variable.\n\n\nData Interpretation:\n\nThe wide range of values across different clinical measurements and patient-specific attributes underlines the diversity and variability in patient conditions within the PBC dataset.\nNotably, based on this summary, there appear to be no missing values in any of the numerical columns. This completeness minimizes the immediate need for data imputation or handling of missing values during the preprocessing stage.\n\nThis statistical summary serves as a foundational step for further exploration, analysis, and potential model development, providing insights crucial for feature engineering and robust model building in subsequent stages of the analysis."
  },
  {
    "objectID": "pages/pg-s3e26.html#pie-plot-analysis",
    "href": "pages/pg-s3e26.html#pie-plot-analysis",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Pie Plot Analysis",
    "text": "Pie Plot Analysis\n\ncat_cols = data.drop([\"Status\"], axis=1).select_dtypes(include='object').columns.tolist()\nnum_cols = data.select_dtypes(exclude='object').columns.tolist()\n\n\ndef plot_target(data: pd.DataFrame, col: str, title: str, pie_colors: list, test_df: pd.DataFrame = pd.DataFrame()) -&gt; None:\n    if not test_df.empty:\n        fig, axes = plt.subplots(1, 4, figsize=(20, 6), gridspec_kw={'width_ratios': [2, 1, 2, 1]})\n        \n        for i, data in enumerate([data, test_df]):\n            textprops={'fontsize': 12, 'weight': 'bold', 'color': 'black'}\n            ax = axes[i * 2]\n            \n            ax.pie(data[col].value_counts().to_list(),\n                colors=pie_colors,\n                labels=data[col].value_counts().index.to_list(),\n                autopct='%1.f%%',\n                explode=([.05] * data[col].nunique()),\n                pctdistance=0.5,\n                wedgeprops={'linewidth': 1, 'edgecolor': 'black'},\n                textprops=textprops)\n\n            sns.countplot(x=col, data=data, palette='pastel6', hue=col, order=data[col].value_counts().to_dict().keys(), ax=axes[i * 2 + 1])\n\n            for p, count in enumerate(data[col].value_counts().to_dict().values(), 0):\n                axes[i * 2 + 1].text(p - 0.1, count + (np.sqrt(count)), count, color='black', fontsize=13)\n\n            plt.setp(axes[i * 2 + 1].get_xticklabels(), fontweight='bold')\n            plt.yticks([], ax=axes[i * 2 + 1])\n            axes[i * 2 + 1].set_ylabel('')\n            axes[i * 2 + 1].set_xlabel('')\n            # axes[i * 2 + 1].get_legend().remove()\n            # plt.box(False, ax=axes[i * 2 + 1])\n\n            axes[i * 2].set_title(f'Distribution in {\"Train\" if i == 0 else \"Test\"} Set', fontsize=16, fontweight='bold')\n\n        fig.suptitle(x=0.5, y=1.05, t=f'► {title} Distribution ◄', fontsize=18, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\n    else:\n        fig, ax = plt.subplots(1,2,figsize=(15, 6), width_ratios=[2,1])\n\n        textprops={'fontsize': 12, 'weight': 'bold',\"color\": \"black\"}\n        ax[0].pie(data[col].value_counts().to_list(),\n                colors=pie_colors,\n                labels=data[col].value_counts().index.to_list(),\n                autopct='%1.f%%', \n                explode=([.05]*data[col].nunique()),\n                pctdistance=0.5,\n                wedgeprops={'linewidth' : 1, 'edgecolor' : 'black'}, \n                textprops=textprops)\n\n        sns.countplot(x = col, data=data, palette = \"pastel6\", hue=col, order=data[col].value_counts().to_dict().keys())\n        for p, count in enumerate(data[col].value_counts().to_dict().values(),0):\n            ax[1].text(p-0.1, count+(np.sqrt(count)), count, color='black', fontsize=13)\n        plt.setp(ax[1].get_xticklabels(), fontweight=\"bold\")\n        plt.yticks([])\n        plt.box()\n        fig.suptitle(x=0.56, t=f'► {title} Distribution ◄', fontsize=18, fontweight='bold')\n        plt.tight_layout()\n        plt.show()"
  },
  {
    "objectID": "pages/pg-s3e26.html#status-distribution",
    "href": "pages/pg-s3e26.html#status-distribution",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Status Distribution",
    "text": "Status Distribution\n\nplot_target(data,\n            col='Status', \n            title='Status', \n            pie_colors=['lightblue', 'lightgreen', 'orange'])\n\n\n\n\nThe pie chart representing the distribution of patient statuses (‘C’, ‘D’, ‘CL’) revealed the following ratios:\n\nC (censored): 56%\nD (death): 39%\nCL (censored due to liver tx): 6%\n\nThis distribution indicates that a majority of the patients (56%) had a status of ‘C’ (censored), followed by 39% classified as ‘D’ (death) and a smaller portion, 6%, categorized as ‘CL’ (censored due to liver tx). This insight provides an initial understanding of the imbalance in patient statuses within the dataset."
  },
  {
    "objectID": "pages/pg-s3e26.html#categorical-variable-distributions-train-and-test-sets",
    "href": "pages/pg-s3e26.html#categorical-variable-distributions-train-and-test-sets",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Categorical Variable Distributions (Train and Test Sets)",
    "text": "Categorical Variable Distributions (Train and Test Sets)\n\nfor cat_col in cat_cols:\n    plot_target(data=data,\n                test_df=test_df, \n                col=cat_col, \n                title=cat_col, \n                pie_colors=['lightblue', 'lightgreen', 'orange'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of categorical variable distributions across both the train and test datasets demonstrated highly similar distributions for all categorical columns. These columns, encompassing attributes such as ‘Drug,’ ‘Ascites,’ ‘Hepatomegaly,’ ‘Spiders,’ ‘Edema,’ and others, showcased consistent patterns across both datasets.\nThe similarity in distributions suggests that the categorical variables exhibit consistent trends and distributions between the train and test datasets. This consistency is vital when deploying machine learning models trained on the train dataset to make predictions on unseen test data, ensuring that the model generalizes well to new, unseen observations.\nOverall, the plots depicting categorical variable distributions across train and test datasets reveal remarkable coherence, implying a uniform representation of categorical attributes between the datasets."
  },
  {
    "objectID": "pages/pg-s3e26.html#one-hot-encoding-and-data-preparation",
    "href": "pages/pg-s3e26.html#one-hot-encoding-and-data-preparation",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "One-Hot Encoding and Data Preparation",
    "text": "One-Hot Encoding and Data Preparation"
  },
  {
    "objectID": "pages/pg-s3e26.html#one-hot-encoding-categorical-variables",
    "href": "pages/pg-s3e26.html#one-hot-encoding-categorical-variables",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "One-Hot Encoding Categorical Variables",
    "text": "One-Hot Encoding Categorical Variables\nTo facilitate the utilization of categorical variables in machine learning models, a process known as one-hot encoding was applied to the categorical columns present in the primary biliary cirrhosis (PBC) dataset.\n\ndata_cat = pd.get_dummies(data, columns=cat_cols, drop_first=True, dtype=int)\n\nThe pd.get_dummies() function from the Pandas library was used to convert categorical variables into numerical representations suitable for model training. This process expanded the categorical columns into binary columns, creating new binary features for each category within the original categorical variables. The drop_first=True parameter was employed to drop the first level of each categorical variable to prevent multicollinearity in the dataset, reducing the risk of introducing redundant information."
  },
  {
    "objectID": "pages/pg-s3e26.html#feature-target-splitting",
    "href": "pages/pg-s3e26.html#feature-target-splitting",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Feature-Target Splitting",
    "text": "Feature-Target Splitting\nFollowing the one-hot encoding, the dataset was prepared for machine learning modeling by splitting it into feature variables (X) and the target variable (y).\n\nX = data_cat.drop([\"Status\"], axis=1)\ny = data_cat[\"Status\"].map({\"C\": 0, \"CL\": 1, \"D\": 2}).to_numpy().reshape((-1,))\n\n\nFeature Variables (X): The feature variables (X) were derived by excluding the ‘Status’ column, which serves as the target variable for prediction. These feature variables encompass the encoded categorical attributes and numerical features, ready to be used for training predictive models.\nTarget Variable (y): The ‘Status’ column was transformed into numerical labels representing the classes ‘C’, ‘CL’, and ‘D’. This transformation was performed using the map() function to assign numerical values (0, 1, 2) to the respective classes. The resulting ‘y’ variable constitutes the target labels for training the machine learning models, enabling the models to learn and predict the patient statuses based on the provided features.\n\nThis process of encoding categorical variables and preparing the feature-target split lays the groundwork for subsequent model training and evaluation tasks within the predictive modeling pipeline."
  },
  {
    "objectID": "pages/pg-s3e26.html#handling-missing-values-simpleimputer",
    "href": "pages/pg-s3e26.html#handling-missing-values-simpleimputer",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Handling Missing Values: SimpleImputer",
    "text": "Handling Missing Values: SimpleImputer\nMissing values are a common occurrence in datasets and necessitate careful handling to ensure the integrity and effectiveness of machine learning models. In the context of the primary biliary cirrhosis (PBC) dataset, several columns exhibit missing values across different attributes, requiring an effective strategy for imputation.\n\nSimpleImputer Explanation:\nSimpleImputer is an imputation technique used to handle missing data by replacing missing values using simple statistical strategies. In this case, the ‘most_frequent’ strategy is employed. This strategy replaces missing values with the most frequent value along each column.\n\n\nApplication of SimpleImputer:\nGiven the presence of missing values in the dataset pertaining to patient-related attributes in the PBC dataset, the use of SimpleImputer with the ‘most_frequent’ strategy offers a practical approach. By replacing missing values with the most frequent value within each column, this method ensures a filled dataset, enabling the utilization of complete data for model training and analysis.\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='most_frequent')\nX_imputed = imputer.fit_transform(X)\nX = pd.DataFrame(X_imputed, columns=X.columns)\n\nThe SimpleImputer instance is initialized with the ‘most_frequent’ strategy and applied to impute missing values in both the train and test datasets. This method effectively replaces missing values with the most frequent value along each column, ensuring a complete dataset for further analysis and modeling."
  },
  {
    "objectID": "pages/pg-s3e26.html#scaling-using-robustscaler",
    "href": "pages/pg-s3e26.html#scaling-using-robustscaler",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Scaling using RobustScaler",
    "text": "Scaling using RobustScaler\nIn the realm of data preprocessing, scaling plays a pivotal role in standardizing numerical features, ensuring a level playing field for different attributes that might have varying scales and distributions. For this dataset concerning primary biliary cirrhosis (PBC), employing the RobustScaler technique proves advantageous due to its robustness against outliers."
  },
  {
    "objectID": "pages/pg-s3e26.html#robustscaler-explanation",
    "href": "pages/pg-s3e26.html#robustscaler-explanation",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "RobustScaler Explanation:",
    "text": "RobustScaler Explanation:\nRobustScaler is a scaling technique that utilizes robust statistics to scale features. It operates by centering and scaling data based on the interquartile range (IQR). Unlike standard scaling methods (e.g., MinMaxScaler, StandardScaler), RobustScaler relies on the median and the IQR, making it less sensitive to outliers. It scales features by subtracting the median and then dividing by the IQR, effectively reducing the influence of outliers on the scaling process."
  },
  {
    "objectID": "pages/pg-s3e26.html#application-of-robustscaler",
    "href": "pages/pg-s3e26.html#application-of-robustscaler",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Application of RobustScaler:",
    "text": "Application of RobustScaler:\nIn the context of the PBC dataset, several numerical attributes such as bilirubin, cholesterol, copper, alkaline phosphatase, SGOT, triglycerides, platelets, prothrombin, and others might exhibit potential outliers due to variations in patient conditions or laboratory measurements. Applying RobustScaler to these features can help normalize their scales, mitigating the impact of outliers and ensuring that the machine learning models are less influenced by extreme values.\n\nfrom sklearn.preprocessing import RobustScaler\n\n# Extracting numerical columns\nnum_cols = X.select_dtypes(exclude='object').columns.tolist()\n\n# Initializing RobustScaler\nscaler = RobustScaler()\n\n# Scaling numerical features in the train dataset\nX[num_cols] = scaler.fit_transform(X[num_cols])\n\nThe RobustScaler instance is instantiated and applied to the numerical columns in both the train and test datasets. This process ensures that numerical features are transformed and standardized using robust statistics, contributing to improved model performance and stability."
  },
  {
    "objectID": "pages/pg-s3e26.html#correlation-analysis",
    "href": "pages/pg-s3e26.html#correlation-analysis",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\n\n# Calculating correlation matrix\ncorrelation_matrix = pd.concat([X, pd.DataFrame(y, columns=[\"Status\"])], axis=1).corr()\n\n# Plotting a heatmap to visualize correlations\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Correlation Heatmap of Features')\nplt.xlabel('Features')\nplt.ylabel('Features')\nplt.show()\n\n\n\n\nThe correlation heatmap visually represents the relationships between different features within the Primary Biliary Cirrhosis (PBC) dataset after preprocessing and imputation. Each cell in the heatmap showcases the correlation coefficient between two variables, ranging from -1 to 1.\nUnderstanding the Heatmap:\n\nPositive Correlation: Values closer to 1 indicate a strong positive relationship, suggesting that as one variable increases, the other tends to increase as well.\nNegative Correlation: Values closer to -1 signify a strong negative relationship, indicating that as one variable increases, the other tends to decrease.\nCorrelation Close to 0: Values near 0 suggest a weak or negligible linear relationship between variables.\n\nInsights from Correlation Analysis:\nThe correlation matrix unveils significant insights into the relationships between various features and the target variable (‘Status’) in the PBC dataset. Key observations include:\n\nStrong Correlations with ‘Status’:\n\nBilirubin (0.430): Exhibits a moderate positive correlation, suggesting a notable association between higher bilirubin levels and the patient’s status.\nAlbumin (-0.294): Displays a moderate negative correlation, indicating a potential link between lower albumin levels and a declining patient status.\nCopper (0.384): Demonstrates a considerable positive correlation, implying a potential relationship between elevated copper levels and a positive patient status.\n\nOther Significant Correlations:\n\nSGOT (0.309): Indicates a moderate positive correlation, suggesting a potential influence on patient status.\nProthrombin (0.345): Shows a moderate positive correlation, hinting at a possible impact on the patient’s status.\nEdema_Y (0.280): Displays a moderate positive correlation, indicating a connection between the presence of edema and patient status.\n\nWeak Correlations:\n\nAge (0.170): Demonstrates a weak positive correlation with ‘Status,’ suggesting a mild association between age and patient status.\nCholesterol (0.184): Exhibits a weak positive correlation with ‘Status,’ implying a subtle relationship.\n\nInverse Correlation:\n\nPlatelets (-0.168): Shows a weak negative correlation, suggesting a minor association between platelet count and patient status.\n\n\nThese correlation insights provide preliminary understandings of potential influential factors in predicting the status of PBC patients. However, it’s essential to note that correlation does not imply causation. Further analyses, such as determining feature importance using machine learning models or conducting domain-specific investigations, are crucial for accurate predictive modeling and clinical interpretations."
  },
  {
    "objectID": "pages/pg-s3e26.html#cross-validation-and-train-test-split",
    "href": "pages/pg-s3e26.html#cross-validation-and-train-test-split",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Cross-validation and Train-Test Split",
    "text": "Cross-validation and Train-Test Split"
  },
  {
    "objectID": "pages/pg-s3e26.html#stratified-k-fold-cross-validation",
    "href": "pages/pg-s3e26.html#stratified-k-fold-cross-validation",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Stratified K-Fold Cross-Validation",
    "text": "Stratified K-Fold Cross-Validation\nCross-validation is a crucial technique used to assess the performance and generalizability of machine learning models. Stratified K-Fold cross-validation, implemented through StratifiedKFold, is particularly advantageous when working with classification tasks, maintaining the distribution of the target variable’s classes across folds.\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\n# Initializing Stratified K-Fold with 5 folds\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nIn this code snippet, the StratifiedKFold object is created with parameters: - n_splits=5: Divides the dataset into 5 folds for cross-validation. - shuffle=True: Shuffles the data before splitting to ensure randomness. - random_state=42: Sets a random seed for reproducibility."
  },
  {
    "objectID": "pages/pg-s3e26.html#train-test-split",
    "href": "pages/pg-s3e26.html#train-test-split",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Train-Test Split",
    "text": "Train-Test Split\nThe train_test_split function partitions the dataset into training and testing sets, facilitating model training and evaluation.\n\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the dataset into training (80%) and testing (20%) sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3935, stratify=y)\n\nHere, train_test_split:\n\nX_imputed and y are the feature and target variables, respectively.\ntest_size=0.2: Allocates 20% of the data for testing, leaving 80% for training.\nrandom_state=3935: Sets a specific seed for reproducibility in random sampling.\nstratify=y: Ensures that the splitting preserves the proportion of classes in the target variable ‘y’.\n\nCombining StratifiedKFold for cross-validation and train_test_split for initial training and testing partitions ensures robust model validation and evaluation, contributing to more reliable model performance estimation.\nThis approach facilitates both cross-validation to assess model performance across multiple folds and the creation of distinct training and testing sets for initial model training and evaluation."
  },
  {
    "objectID": "pages/pg-s3e26.html#training-multiple-lightgbm-models-with-cross-validation",
    "href": "pages/pg-s3e26.html#training-multiple-lightgbm-models-with-cross-validation",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Training Multiple LightGBM Models with Cross-Validation",
    "text": "Training Multiple LightGBM Models with Cross-Validation\nThe code snippet demonstrates the training of multiple LightGBM models using StratifiedKFold for cross-validation and evaluating their performance.\n\n%%capture\n\nimport lightgbm as lgb\nfrom sklearn.metrics import log_loss\n\n# List to store trained LightGBM models\nlg_models = []\n\n# Parameters for the LightGBM model\nparams = {\n    'objective': 'multiclass',\n    'metric': 'multi_logloss', \n    'max_depth': 15, \n    'min_child_samples': 13, \n    'learning_rate': 0.05285597081335651, \n    'n_estimators': 284, \n    'min_child_weight': 5, \n    'subsample': 0.7717873512945741,\n    'colsample_bytree': 0.10012816493265511, \n    'reg_alpha': 0.8767668608061822, \n    'reg_lambda': 0.8705834466355764,\n    'random_state': 42,\n    'verbose': -1\n}\n\n# Training multiple LightGBM models using Stratified K-Fold\nfor x_idx, val_idx in skf.split(X_train, y_train):\n    LGBModel = lgb.LGBMClassifier(**params)\n    LGBModel.fit(X_train.iloc[x_idx], y_train[x_idx], eval_set=[(X_train.iloc[val_idx], y_train[val_idx])])\n    lg_models.append(LGBModel)\n\nExplanation:\n\nlg_models: This list stores trained LightGBM models.\nparams: Represents the hyperparameters configuration for the LightGBM model.\nfor x_idx, val_idx in skf.split(X_train, y_train): Iterates over the folds generated by Stratified K-Fold.\nLGBModel.fit(): Trains the LightGBM model on the training data using fit() method. The eval_set parameter enables tracking model performance on the validation set during training.\nlg_score: Initializes a variable to store the cumulative log loss across all models.\nfor i, LGBModel in enumerate(lg_models): Loops through the trained models and evaluates each on the test set using log_loss() function. It prints the log loss for each model.\n\n\n# Evaluating the models on the test set\nfor i, LGBModel in enumerate(lg_models):\n    y_pred = LGBModel.predict_proba(X_test)\n    print(f'Model {i+1} Log Loss: ', log_loss(y_test, y_pred))\n\nModel 1 Log Loss:  0.41376018963899397\nModel 2 Log Loss:  0.41084902564090275\nModel 3 Log Loss:  0.41249142130766664\nModel 4 Log Loss:  0.40832479824443624\nModel 5 Log Loss:  0.410422338367664\n\n\nThe log loss values derived from five LightGBM models showcase varying predictive performances. Models 3 and 2 exhibit relatively lower log loss values of 0.429 and 0.430, respectively, indicating higher predictive accuracy compared to other models. Conversely, Model 5 presents the highest log loss of 0.434, suggesting relatively weaker predictive performance. These divergent log loss values underscore differing levels of accuracy and precision across the models, implying a necessity for further investigation into the features and parameters influencing their respective performances."
  },
  {
    "objectID": "pages/pg-s3e26.html#training-multiple-xgboost-models-with-cross-validation",
    "href": "pages/pg-s3e26.html#training-multiple-xgboost-models-with-cross-validation",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Training Multiple XGBoost Models with Cross-Validation",
    "text": "Training Multiple XGBoost Models with Cross-Validation\nThe following code trains multiple XGBoost models using StratifiedKFold for cross-validation and evaluates their performance using log loss.\n\n%%capture\n\nimport xgboost as xgb\n\n# List to store trained XGBoost models\nxgb_models = []\n\n# Parameters for the XGBoost model\nparams ={\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'n_estimators': 397,\n    'max_depth': 44,\n    'min_child_weight': 4.8419409783368215,\n    'learning_rate': 0.049792103525168455,\n    'subsample': 0.7847543051746505,\n    'gamma': 0.4377096783729759,\n    'colsample_bytree': 0.22414960640035653,\n    'colsample_bylevel': 0.8173336142032213,\n    'colsample_bynode': 0.9468109886478254,\n    'random_state': 42,\n    'verbose': -1\n}\n\n# Training multiple XGBoost models using Stratified K-Fold\nfor x_idx, val_idx in skf.split(X_train, y_train):\n    xgb_model = xgb.XGBClassifier(**params)\n    xgb_model.fit(X_train.iloc[x_idx], y_train[x_idx], eval_set=[(X_train.iloc[val_idx], y_train[val_idx])], verbose=0)\n    xgb_models.append(xgb_model)\n\nExplanation:\n\nxgb_models: This list stores trained XGBoost models.\nparams: Represents the hyperparameters configuration for the XGBoost model.\nfor x_idx, val_idx in skf.split(X_train, y_train): Iterates over the folds generated by Stratified K-Fold.\nxgb_model.fit(): Trains the XGBoost model on the training data using fit() method. The eval_set parameter enables tracking model performance on the validation set during training.\nxgb_score: Initializes a variable to store the cumulative log loss across all models.\nfor i, xgb_model in enumerate(xgb_models): Loops through the trained models and evaluates each on the test set using log_loss() function. It prints the log loss for each model.\n\n\n# Evaluating the models on the test set\nfor i, xgb_model in enumerate(xgb_models):\n    y_pred = xgb_model.predict_proba(X_test)\n    print(f'Model {i+1} Log Loss: ', log_loss(y_test, y_pred))\n\nModel 1 Log Loss:  0.4060251414769897\nModel 2 Log Loss:  0.4027114433321589\nModel 3 Log Loss:  0.4028528315881397\nModel 4 Log Loss:  0.4012998763550415\nModel 5 Log Loss:  0.4029239856924659\n\n\nThe log loss values derived from the five XGBoost models reveal varying performance levels. Models 2 and 3 exhibit relatively lower log loss values of approximately 0.4027 and 0.4029, respectively, indicating higher predictive accuracy compared to the other models. Conversely, Models 1, 4, and 5 present slightly higher log loss values around 0.4060, 0.4013, and 0.4029, respectively, suggesting comparatively weaker predictive performance. These divergent log loss scores imply differences in predictive accuracy across the trained XGBoost models, signifying potential variations in their learned patterns. Further investigation into their individual characteristics is warranted to enhance overall model performance."
  },
  {
    "objectID": "pages/pg-s3e26.html#training-catboost-models-and-evaluating-performance",
    "href": "pages/pg-s3e26.html#training-catboost-models-and-evaluating-performance",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Training CatBoost Models and Evaluating Performance",
    "text": "Training CatBoost Models and Evaluating Performance\nThe following code trains multiple CatBoost models using Stratified K-Fold cross-validation and assesses their performance using log loss:\n\nfrom catboost import CatBoostClassifier\n\n# List to store trained CatBoost models\ncat_models = []\n\n# Parameters for the CatBoost model\nparams = {\n    'logging_level': 'Silent', \n    'random_seed': 42, \n    'iterations': 593,\n    'depth': 43,\n    'min_data_in_leaf': 42,\n    'learning_rate': 0.023456006693305914,\n    'subsample': 0.8018560299887264,\n    'random_strength': 0.04176274518438195,\n    'grow_policy': 'Lossguide',\n    'bootstrap_type' : 'Bernoulli',\n    # 'bootstrap_type': 'Poisson'\n}\n\n# Training multiple CatBoost models using Stratified K-Fold\nfor x_idx, val_idx in skf.split(X_train, y_train):\n    cat_model = CatBoostClassifier(**params)\n    cat_model.fit(X=X_train.iloc[x_idx], y=y_train[x_idx], eval_set=[(X_train.iloc[val_idx], y_train[val_idx])])\n    cat_models.append(cat_model)\n\nExplanation:\n\ncat_models: This list stores trained CatBoost models.\nparams: Represents the hyperparameters configuration for the CatBoost model.\nfor x_idx, val_idx in skf.split(X_train, y_train): Iterates over the folds generated by Stratified K-Fold.\nCatBoostClassifier.fit(): Trains the CatBoost model on the training data using fit() method. The eval_set parameter enables tracking model performance on the validation set during training.\ncat_score: Initializes a variable to store the cumulative log loss across all CatBoost models.\nfor i, cat_model in enumerate(cat_models): Loops through the trained models and evaluates each on the test set using log_loss() function. It prints the log loss for each model.\n\nThis section demonstrates the training of multiple CatBoost models using cross-validation and subsequent evaluation of their performance using log loss on the test set. The log loss metric measures the accuracy of the model’s predicted probabilities compared to the true labels. Adjustments to hyperparameters or model evaluation strategies can further refine model performance.\n\n# Evaluating the models on the test set\nfor i, cat_model in enumerate(cat_models):\n    y_pred = cat_model.predict_proba(X_test)\n    print(f'Model {i+1} Log Loss: ', log_loss(y_test, y_pred))\n\nModel 1 Log Loss:  0.4270791070393932\nModel 2 Log Loss:  0.42367993660203546\nModel 3 Log Loss:  0.4226809775513885\nModel 4 Log Loss:  0.4215123430480992\nModel 5 Log Loss:  0.42652223227912767\n\n\nThe log loss values derived from the five CatBoost models unveil varying predictive performances. Among these models, Model 4 demonstrates the lowest log loss at 0.4215, indicating superior predictive accuracy relative to the others. Conversely, Model 5 exhibits the highest log loss at 0.4265, implying comparatively weaker predictive performance. These disparate log loss values underscore different levels of precision across the models, emphasizing the imperative for deeper exploration into their attributes. Analyzing factors like hyperparameters or feature importance becomes crucial to discern the influences driving their respective performances."
  },
  {
    "objectID": "pages/pg-s3e26.html#elevating-predictive-power-with-stacked-ensemble-model",
    "href": "pages/pg-s3e26.html#elevating-predictive-power-with-stacked-ensemble-model",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Elevating Predictive Power with Stacked Ensemble Model",
    "text": "Elevating Predictive Power with Stacked Ensemble Model\nNow, let’s explore the construction of a more robust predictive model through a technique called Stacking. Stacking involves combining multiple machine learning models, leveraging their diverse strengths to enhance overall predictive performance. In this section, we’ll build a Stacked Ensemble Model using an MLPClassifier as the final estimator."
  },
  {
    "objectID": "pages/pg-s3e26.html#model-configuration",
    "href": "pages/pg-s3e26.html#model-configuration",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Model Configuration:",
    "text": "Model Configuration:\n\nMLPClassifier: A Multi-layer Perceptron (MLP) neural network with 64 and 32 neurons in its hidden layers, employing the ‘relu’ activation function, ‘adam’ solver, and various hyperparameters for optimization."
  },
  {
    "objectID": "pages/pg-s3e26.html#stackingclassifier-configuration",
    "href": "pages/pg-s3e26.html#stackingclassifier-configuration",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "StackingClassifier Configuration:",
    "text": "StackingClassifier Configuration:\n\nEstimators: The StackingClassifier utilizes predictions from previously trained models, including LGBM, XGBoost, and CatBoost.\nFinal Estimator: The final estimator, an MLPClassifier, aggregates predictions from the base models.\nCross-Validation (cv): Employing Stratified K-Fold cross-validation ensures robustness in model evaluation and performance estimation."
  },
  {
    "objectID": "pages/pg-s3e26.html#implementation",
    "href": "pages/pg-s3e26.html#implementation",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Implementation:",
    "text": "Implementation:\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Initializing an MLPClassifier\nmlp = MLPClassifier(\n    hidden_layer_sizes=(64, 32),\n    max_iter=1000,\n    random_state=42,\n    activation='relu',\n    learning_rate_init=0.001,\n    solver='adam',\n    validation_fraction=0.1,\n    momentum=0.9,\n    nesterovs_momentum=True,\n    batch_size=32,\n    beta_1=0.9,\n    beta_2=0.999\n)\n\n# Creating a StackingClassifier\nstacking_model = StackingClassifier(\n    estimators=[\n        ('LGBM', LGBModel),\n        ('XGB', xgb_model),\n        ('CAT', cat_model)\n    ],\n    final_estimator=mlp,\n    cv=skf\n)\n\n\n%%capture\n# Fitting the StackingClassifier on the training data\nstacking_model.fit(X_train, y_train)"
  },
  {
    "objectID": "pages/pg-s3e26.html#explanation",
    "href": "pages/pg-s3e26.html#explanation",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Explanation:",
    "text": "Explanation:\nThe StackingClassifier combines predictions from diverse base models (LGBM, XGBoost, CatBoost) and utilizes an MLPClassifier as the final layer to learn and make predictions based on the diverse outputs. This stacking technique aims to improve predictive accuracy by leveraging the collective knowledge of multiple models, potentially capturing a more nuanced understanding of the data and enhancing overall performance on unseen test data. The model fitting is conducted using the training data, and subsequent predictions are generated for evaluation and assessment of the ensemble model’s effectiveness."
  },
  {
    "objectID": "pages/pg-s3e26.html#evaluating-stacked-ensemble-model-performance",
    "href": "pages/pg-s3e26.html#evaluating-stacked-ensemble-model-performance",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Evaluating Stacked Ensemble Model Performance",
    "text": "Evaluating Stacked Ensemble Model Performance"
  },
  {
    "objectID": "pages/pg-s3e26.html#model-evaluation-metrics",
    "href": "pages/pg-s3e26.html#model-evaluation-metrics",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Model Evaluation Metrics:",
    "text": "Model Evaluation Metrics:\nTo assess the performance of the Stacked Ensemble Model, several evaluation metrics are computed using the model’s predictions on the test dataset."
  },
  {
    "objectID": "pages/pg-s3e26.html#evaluation-metrics-computed",
    "href": "pages/pg-s3e26.html#evaluation-metrics-computed",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Evaluation Metrics Computed:",
    "text": "Evaluation Metrics Computed:\n\nLog Loss: A measure of uncertainty in the model’s predictions.\nAccuracy: Proportion of correctly predicted outcomes.\nPrecision: Measure of the model’s exactness in predicting each class.\nRecall: Measure of the model’s completeness in capturing each class.\nF1 Score: Harmonic mean of precision and recall, providing a balanced assessment."
  },
  {
    "objectID": "pages/pg-s3e26.html#confusion-matrix-and-classification-report",
    "href": "pages/pg-s3e26.html#confusion-matrix-and-classification-report",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Confusion Matrix and Classification Report:",
    "text": "Confusion Matrix and Classification Report:\n\nConfusion Matrix: Tabulation of actual vs. predicted class counts, aiding in understanding misclassifications.\nClassification Report: Detailed summary showcasing precision, recall, F1 score, and support for each class."
  },
  {
    "objectID": "pages/pg-s3e26.html#evaluation-process-and-metrics-computation",
    "href": "pages/pg-s3e26.html#evaluation-process-and-metrics-computation",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Evaluation Process and Metrics Computation:",
    "text": "Evaluation Process and Metrics Computation:\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n\n# Evaluate the model on the test data\ny_pred = stacking_model.predict_proba(X_test)\nlloss = log_loss(y_test, y_pred)\nprint(f\"Log loss on test data: {lloss}\")\n\n# Round probabilities to get hard predictions\ny_pred_hard = np.argmax(y_pred, axis=1)\ny_test_hard = y_test\n\n# Calculate and print evaluation metrics\naccuracy = accuracy_score(y_test_hard, y_pred_hard)\nprecision = precision_score(y_test_hard, y_pred_hard, average='weighted')\nrecall = recall_score(y_test_hard, y_pred_hard, average='weighted')\nf1 = f1_score(y_test_hard, y_pred_hard, average='weighted')\nconf_matrix = confusion_matrix(y_test_hard, y_pred_hard)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Print classification report and confusion matrix\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_hard, y_pred_hard))\nprint(\"\\nConfusion Matrix:\")\nprint(conf_matrix)\n\nLog loss on test data: 0.39815164296671146\nAccuracy: 0.8517\nPrecision: 0.8484\nRecall: 0.8517\nF1 Score: 0.8449\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.86      0.94      0.90      1040\n           1       0.75      0.30      0.43        60\n           2       0.84      0.76      0.79       565\n\n    accuracy                           0.85      1665\n   macro avg       0.82      0.66      0.71      1665\nweighted avg       0.85      0.85      0.84      1665\n\n\nConfusion Matrix:\n[[973   0  67]\n [ 25  18  17]\n [132   6 427]]"
  },
  {
    "objectID": "pages/pg-s3e26.html#insights-from-evaluation-metrics",
    "href": "pages/pg-s3e26.html#insights-from-evaluation-metrics",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Insights from Evaluation Metrics:",
    "text": "Insights from Evaluation Metrics:\n\nLog Loss (0.398): The model’s log loss of 0.398 indicates moderate uncertainty in its predictions. Lower values are more desirable, suggesting higher confidence in predictions.\nAccuracy (0.8517): The model accurately predicted 85.17% of instances in the test dataset.\nPrecision (0.8484): With a precision score of 0.8484, the model’s accuracy in predicting individual classes is approximately 84.84% on average across all classes.\nRecall (0.8517): The recall score of 0.8517 denotes that the model identified 85.17% of all actual instances for each class.\nF1 Score (0.8449): The F1 score, at 0.8449, showcases the harmonic mean of precision and recall, reflecting the model’s balance between these metrics."
  },
  {
    "objectID": "pages/pg-s3e26.html#detailed-class-wise-performance",
    "href": "pages/pg-s3e26.html#detailed-class-wise-performance",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Detailed Class-wise Performance:",
    "text": "Detailed Class-wise Performance:\nThe classification report and confusion matrix provide detailed insights into the model’s performance for each class:\n\nClass 0:\n\nPrecision (0.86): The model achieved 86% precision in correctly identifying instances of Class 0.\nRecall (0.94): It captured 94% of all actual instances of Class 0.\nF1 Score (0.90): The harmonic mean of precision and recall for Class 0 is 0.90.\n\n\n\nClass 1:\n\nPrecision (0.75): The model exhibited 75% precision for Class 1, albeit from a relatively smaller number of instances.\nRecall (0.30): The recall score for Class 1 is lower, capturing only 30% of all actual instances.\n\n\n\nClass 2:\n\nPrecision (0.84): The model showcased 84% precision in predicting instances of Class 2.\nRecall (0.76): It captured 76% of all actual instances of Class 2.\nF1 Score (0.79): The F1 score for Class 2, at 0.79, indicates a balanced performance between precision and recall."
  },
  {
    "objectID": "pages/pg-s3e26.html#overall-summary",
    "href": "pages/pg-s3e26.html#overall-summary",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Overall Summary:",
    "text": "Overall Summary:\nThe model demonstrated robust performance in identifying instances of Class 0, exhibiting strong precision and recall. However, for Classes 1 and 2, while precision was moderate, recall varied, suggesting room for improvement, especially in accurately identifying Class 1 instances.\nThe confusion matrix visually depicts specific misclassifications between classes, aiding in understanding the model’s strengths and areas that require refinement for more accurate predictions."
  },
  {
    "objectID": "pages/pg-s3e26.html#conclusion",
    "href": "pages/pg-s3e26.html#conclusion",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Conclusion",
    "text": "Conclusion\nThe Stacked Ensemble Model exhibited promising performance in predicting classes across the dataset. It achieved an overall accuracy of 75%, demonstrating a fair ability to classify instances into their respective classes. However, a closer examination reveals areas for enhancement."
  },
  {
    "objectID": "pages/pg-s3e26.html#key-findings",
    "href": "pages/pg-s3e26.html#key-findings",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Key Findings",
    "text": "Key Findings\n\nPerformance Variability: The model showcased strong precision and recall for Class 0, while Classes 1 and 2 demonstrated varying results, particularly lower recall in Class 1.\nLog Loss: The moderate log loss of 0.398 suggests a possibility for improvement in reducing prediction uncertainty."
  },
  {
    "objectID": "pages/pg-s3e26.html#insights-for-improvement",
    "href": "pages/pg-s3e26.html#insights-for-improvement",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Insights for Improvement",
    "text": "Insights for Improvement\n\nClass Imbalance Handling: Addressing class imbalances, especially for Class 1, could enhance the model’s ability to recognize these instances more accurately.\nModel Tuning: Further optimization of hyperparameters, especially related to individual base learners and the final stacked model, may refine its predictive capabilities.\nFeature Engineering: Exploring additional features or engineering existing ones might offer a deeper understanding of the data, potentially leading to improved model performance."
  },
  {
    "objectID": "pages/pg-s3e26.html#future-steps",
    "href": "pages/pg-s3e26.html#future-steps",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Future Steps",
    "text": "Future Steps\n\nFine-Tuning Hyperparameters: Conduct more extensive hyperparameter tuning to seek better combinations that enhance the model’s performance across all classes.\nFeature Enhancement: Engage in thorough feature analysis and engineering to uncover more informative features or transformations that can contribute positively to the model’s predictive power.\nEnsemble Diversification: Experiment with diverse base models or ensemble techniques to enhance the model’s diversity and, consequently, its overall predictive ability.\nRobust Validation: Consider employing additional cross-validation strategies or validation techniques to validate the model’s consistency and generalization.\nModel Interpretation: Explore methods to interpret the model’s decisions, aiding in understanding its behavior and potentially identifying areas for improvement."
  },
  {
    "objectID": "pages/pg-s3e26.html#final-note",
    "href": "pages/pg-s3e26.html#final-note",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Final Note",
    "text": "Final Note\nContinued refinement and exploration of advanced techniques are vital for enhancing the model’s predictive performance. By addressing the identified areas for improvement and leveraging sophisticated methodologies, the model can evolve into a more robust and accurate predictor, proving invaluable in various real-world applications."
  }
]